{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WiseWheels - what is behind it\n",
    "Below I will record the data processing, feature engineering, and model selection for the WiseWheels project. The raw trip data (12 .csv files for 2016) is already preprocessed and pickled as a .pkl file. Other related files such as stations, weather, Yelp (in csv form) are saved in other location, and loaded on the fly.\n",
    "### Premeable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os, sys, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "## my function library\n",
    "import function_lib as fun\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 6)\n",
    "\n",
    "data_year = '2016'\n",
    "## get the path two levels up\n",
    "data_path = '/'.join(os.getcwd().split('/')[:-2]) + '/python projects/CitiBike/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the \"mega data\" in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Reading the raw pickled data...')\n",
    "trips = pd.read_pickle(data_path + 'citibike_'+ data_year +'.pkl')\n",
    "print('Reading the raw pickled data done!')\n",
    "## only keep the Subscriber\n",
    "trips = trips[trips['usertype'] == 'Subscriber']\n",
    "## let's build the training set\n",
    "features = ['starttime', 'start station id',\n",
    "           'birth year', 'gender', 'date', 'hour', 'weekday']\n",
    "target = ['tripduration', 'end station id']\n",
    "trips_lite = trips[features + target]\n",
    "trips_lite['day of week'] = trips_lite['date'].dt.dayofweek\n",
    "trips_lite['age'] = 2016 - trips_lite['birth year']\n",
    "trips_lite.head()\n",
    "\n",
    "## get the end neighborhood\n",
    "df_stations = pd.read_csv('./data/NYC_bike_stations_v7.csv')\n",
    "trips_lite = pd.merge(trips_lite, df_stations[['id', 'neighborhood']],\n",
    "                     left_on = 'end station id', right_on = 'id', how = 'left')\n",
    "trips_lite = trips_lite.rename(columns = {'neighborhood': 'end neighborhood'})\n",
    "\n",
    "## merage with start station information\n",
    "trips_lite = pd.merge(trips_lite, df_stations[['id', 'neighborhood',\n",
    "            'shortest_dist','total_slots','closest_income',\n",
    "            'closest_pop','nearest_3','nearest_5']], \n",
    "         left_on = 'start station id', right_on = 'id', how = 'left')\n",
    "trips_lite = trips_lite.rename(columns = {'neighborhood': 'start neighborhood'})\n",
    "\n",
    "## add the info from yelp\n",
    "df_yelp = pd.read_csv('./data/station_attributes.csv')\n",
    "print('Yelp data shape:', df_yelp.shape)\n",
    "## remove the 'name'\n",
    "df_yelp = df_yelp.drop(['name'], axis = 1)\n",
    "print('Yelp data headers: ', df_yelp.columns)\n",
    "trips_lite = pd.merge(trips_lite, df_yelp, left_on = 'start station id', right_on = 'id', \n",
    "     how = 'left')\n",
    "\n",
    "## get the weather information, for each day\n",
    "weather = pd.read_csv(data_path + 'weather_lite.csv', parse_dates = ['date'], infer_datetime_format = True)\n",
    "trips_lite = trips_lite.merge(weather, on = 'date', how = 'left')\n",
    "trips_lite.columns\n",
    "\n",
    "## save 'some' of the data. If I try to save it all, it takes too much time\n",
    "## somehow pickle throw me some error...\n",
    "# trips_lite.sample(1000000).to_csv('./data/ready_to_train_2016_trips_lite.csv', index = False) \n",
    "print('data loaded.')\n",
    "trips_lite.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's start training!\n",
    "### First let me take only part the 10 million+ samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## taking only 1%, which is about 100,000 samples\n",
    "data = trips_lite.sample(int(1 / 100 * trips_lite.shape[0]))\n",
    "print('Shape of the dataset is:', data.shape)\n",
    "## let me check for the nan entris\n",
    "print('Number of n/a entries are :', data.isnull().sum())\n",
    "## drop the rows with any nan entry\n",
    "data = data.dropna()\n",
    "## delete the original file to free some space\n",
    "del trips_lite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then let me load the station info, which will be used for new features, and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df_stations.columns\n",
    "new_features = ['id']\n",
    "for elem in cols:\n",
    "    if elem.startswith('to ') or elem.startswith('freq to '):\n",
    "        new_features.append(elem)\n",
    "df_tmp = df_stations[new_features]\n",
    "print('bike station df shape: {}'.format(df_tmp.shape))\n",
    "# add these new features to the data\n",
    "data = pd.merge(data, df_tmp, left_on = 'start station id', right_on = 'id', how = 'inner')\n",
    "print('data df shape: {}'.format(data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['start station id', 'hour', 'weekday', 'day of week',\n",
    "            'age', 'gender', \n",
    "            'TAVG', 'PRCP', 'SNOW',\n",
    "            'shortest_dist','total_slots','nearest_3','nearest_5',\n",
    "            'closest_pop', \n",
    "            'closest_income', \n",
    "            # =========== following are from yelp ===========\n",
    "            #'apartments', 'coffee', 'condominiums', 'convenience',\n",
    "            #'elementaryschools', 'highschools', 'metrostations', 'pubs',\n",
    "            #'restaurant', 'trainstations', 'artmuseums', 'auto', 'banks',\n",
    "            #'bikerentals', 'businessfinancing', 'dryclean', 'gyms', 'hair',\n",
    "            #'landmarks', 'laundromat', 'othersalons', 'paydayloans', 'preschools',\n",
    "           ]\n",
    "if 'id' in new_features:\n",
    "    new_features.remove('id')  # remove the key column \n",
    "features.extend(new_features)  # whether to add the new station features: dists to all neighbors\n",
    "target = ['end neighborhood']\n",
    "data = data[features + target]\n",
    "print('The shape of the data is:', data.shape)\n",
    "data.sample(5).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one_hoc encoding for the categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns=[\n",
    "                                  'gender',\n",
    "                                  'start station id',  'weekday', 'day of week'])\n",
    "print('The shape of the data after one_hoc encoding is:', data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the classifiers, and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "## choose what model to train\n",
    "logistic_model = False\n",
    "svm_model      = False\n",
    "rf_model       = False\n",
    "xgb_model      = False\n",
    "nb_model       = False\n",
    "\n",
    "seed = 42\n",
    "train, test = train_test_split(data, test_size = 0.2, random_state = seed)\n",
    "features = data.columns.tolist()\n",
    "features.remove(*target)\n",
    "\n",
    "# scaling the features\n",
    "scaler = StandardScaler()\n",
    "X_train, y_train = train[features], train[target].values.ravel()\n",
    "X_test, y_test = test[features], test[target].values.ravel()\n",
    "scaler.fit(X_train)  # Don't cheat - fit only on training data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # apply same transformation to test data\n",
    "print('number of training samples: {}'.format(len(X_train_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression with SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ==================== Logistic classification ====================\n",
    "if logistic_model:\n",
    "    gridsearchLogistic = False  # whether to do grid search\n",
    "    n_iter = 50  # number of iteration of SGD\n",
    "    t_start = time.time()  # for timing purpose\n",
    "    print('number of epochs:', n_iter)\n",
    "    if not gridsearchLogistic:        \n",
    "        ## to repeat few times and get the statistics\n",
    "        n_iters = [50]*5\n",
    "        train_scores = []\n",
    "        test_scores = []\n",
    "        for i, n_iter in enumerate(n_iters):\n",
    "            print('working on {} of {}...'.format(i+1, len(n_iters)))\n",
    "            single_clf = SGDClassifier(loss = 'log', n_iter=n_iter, n_jobs = -1, \n",
    "                                       verbose = 0, alpha = 0.0001,\n",
    "                                       penalty = 'l2')\n",
    "            single_clf.fit(X_train_scaled, y_train)\n",
    "            train_score = single_clf.score(X_train_scaled, y_train)\n",
    "            test_score  = single_clf.score(X_test_scaled, y_test)\n",
    "            print('train scroe: {}'.format(train_score))\n",
    "            print('test scroe: {}'.format(test_score))\n",
    "            train_scores.append(train_score)\n",
    "            test_scores.append(test_score)\n",
    "            # fun.plot_conf_mat(clf=single_clf, X_test=X_test_scaled, y_test=y_test, N=5)\n",
    "        print('mean and std for training are: {}, and {}'\n",
    "             .format(np.array(train_scores).mean(), np.array(train_scores).std()))\n",
    "        print('mean and std for test are: {}, and {}'\n",
    "             .format(np.array(test_scores).mean(), np.array(test_scores).std()))\n",
    "            \n",
    "    else:  # let me do a grid search on some parameters\n",
    "        parameters = {'alpha': 10.0**-np.arange(1,7)}\n",
    "        single_clf = SGDClassifier(loss = 'log', n_iter=n_iter, n_jobs = -1, verbose = 0,\n",
    "                                   penalty = 'l2')\n",
    "        clf = GridSearchCV(single_clf, parameters, verbose = 1)\n",
    "        clf.fit(X_train_scaled, y_train)\n",
    "        print('best scroe is: {}'.format(clf.best_score_))\n",
    "    print('Elapsed time is {} min'.format((time.time() - t_start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine with SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ==================== SVM ====================\n",
    "from sklearn.svm import SVC\n",
    "if svm_model:\n",
    "    n_iter = 50  # number of iterations\n",
    "    gridsearchSVM = False  # whether to do grid search\n",
    "    t_start = time.time()  # for timing purpose\n",
    "    print('number of epochs:', n_iter)\n",
    "    if not gridsearchSVM:\n",
    "        ## to repeat few times and get the statistics\n",
    "        n_iters = [50]*5\n",
    "        train_scores = []\n",
    "        test_scores = []\n",
    "        for i, n_iter in enumerate(n_iters):\n",
    "            print('working on {} of {}...'.format(i+1, len(n_iters)))\n",
    "            single_svm = SGDClassifier(loss = 'squared_hinge', n_iter=n_iter, n_jobs = 6, \n",
    "                                           verbose = 0, alpha = 0.0001, penalty = 'l1')\n",
    "            single_svm.fit(X_train_scaled, y_train)\n",
    "            train_score = single_svm.score(X_train_scaled, y_train)\n",
    "            test_score  = single_svm.score(X_test_scaled, y_test)\n",
    "            print('training scroe: {}'.format(train_score))\n",
    "            print('test scroe: {}'.format(test_score))\n",
    "            train_scores.append(train_score)\n",
    "            test_scores.append(test_score)\n",
    "        print('mean and std for training are: {}, and {}'\n",
    "             .format(np.array(train_scores).mean(), np.array(train_scores).std()))\n",
    "        print('mean and std for test are: {}, and {}'\n",
    "             .format(np.array(test_scores).mean(), np.array(test_scores).std()))\n",
    "    else:\n",
    "        parameters = {'alpha': 10.0**-np.arange(1,7), \n",
    "                      'loss': ['hinge', 'squared_hinge'],\n",
    "                      'penalty': ['l1', 'l2']\n",
    "                     }\n",
    "        single_svm = SGDClassifier(loss = 'squared_hinge', n_iter=n_iter, n_jobs = 4,\n",
    "                                   verbose = 0)\n",
    "        clf = GridSearchCV(single_svm, parameters, verbose = 1)\n",
    "        clf.fit(X_train_scaled, y_train)\n",
    "        print('best scroe is: {}'.format(clf.best_score_))\n",
    "        print('best model is: {}'.format(clf.best_estimator_))\n",
    "    print('Elapsed time is {} min'.format((time.time() - t_start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Random Forest ====================\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "if rf_model:\n",
    "    t_start = time.time()\n",
    "    gridsearchRF = False\n",
    "    if not gridsearchRF:\n",
    "        max_depth = 10\n",
    "        n_iters = [50]*5\n",
    "        train_scores = []\n",
    "        test_scores = []\n",
    "        for i, n_iter in enumerate(n_iters):\n",
    "            print('working on {} of {}...'.format(i+1, len(n_iters)))\n",
    "            rf_single = RandomForestClassifier(n_estimators=200,\n",
    "                                                       max_depth=max_depth, n_jobs = 1,\n",
    "                                                       verbose = True)\n",
    "            rf_single.fit(X_train, y_train)\n",
    "            train_score = rf_single.score(X_train, y_train)\n",
    "            test_score  = rf_single.score(X_test, y_test)\n",
    "            print('train scroe: {} \\n'.format(train_score))\n",
    "            print('test scroe: {} \\n'.format(test_score))\n",
    "            train_scores.append(rf_single.score(X_train, y_train))\n",
    "            test_scores.append(rf_single.score(X_test, y_test))\n",
    "        \n",
    "        print('mean and std for training are: {}, and {}'\n",
    "             .format(np.array(train_scores).mean(), np.array(train_scores).std()))\n",
    "        print('mean and std for test are: {}, and {}'\n",
    "             .format(np.array(test_scores).mean(), np.array(test_scores).std()))\n",
    "        importances = [[c, i] for c, i in zip(\n",
    "            data.columns, rf_single.feature_importances_)];\n",
    "        importances = sorted(importances, key=lambda x: x[1], reverse = True);\n",
    "        print('Feature importances:')\n",
    "        for j in range(20):\n",
    "            print(importances[j][0], importances[j][1])\n",
    "        model_for_output = rf_single\n",
    "    else:\n",
    "        parameters = {'max_depth': [x for x in range(10, 41, 5)]}\n",
    "        single_clf = RandomForestClassifier(n_estimators = 400, n_jobs = 1, verbose = False)\n",
    "        clf = GridSearchCV(single_clf, parameters, verbose = 2)\n",
    "        clf.fit(X_train, y_train)\n",
    "        print('best model is : {}.\\n'.format(clf.best_estimator_))\n",
    "        print('best train scroe is: {} \\n'.format(clf.best_score_))\n",
    "        print('test score is: {} \\n'.format(clf.best_estimator_.score(X_test, y_test)))\n",
    "        importances = [[c, i] for c, i in zip(\n",
    "            data.columns, clf.best_estimator_.feature_importances_)];\n",
    "        importances = sorted(importances, key=lambda x: x[1], reverse = True);\n",
    "        for j in range(10):\n",
    "            print(importances[j][0], importances[j][1])\n",
    "        # model_for_output = clf.best_estimator_\n",
    "        \n",
    "    X_sample = pd.Series(0, index = X_train.columns)\n",
    "    model_name = 'Random_Forest.pkl'\n",
    "    # check for the prediction\n",
    "    print(\"\\nLet's look at one test case:\")\n",
    "    idx = 1090\n",
    "    debug_input_X, debug_y = X_test.iloc[idx,:], y_test[idx]      \n",
    "    if not gridsearchRF:\n",
    "        fun.get_proba(clf = model_for_output, X_scaled = debug_input_X, y_true = debug_y, N = 5)\n",
    "        # pickle.dump([model_for_output, X_sample, debug_input_X, debug_y], open(model_name, 'wb'))\n",
    "        ## plot confuction matrix\n",
    "        # fun.plot_conf_mat(clf = model_for_output, X_test = X_test, y_test = y_test, N = 5)\n",
    "        # fun.plot_conf_mat_bokeh(clf=rf_single, X_test=X_test, y_test=y_test)\n",
    "    print('Elapsed time is {} min'.format((time.time() - t_start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Trees (xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ========================== xgboost ==================================\n",
    "if xgb_model:\n",
    "    import xgboost as xgb\n",
    "    ## the sklearn wrapper of xgboost seems to be problematic in this case:\n",
    "    ## it simply doesn't stop! Therefore I am folding back to the most basic DMatrix\n",
    "    ## in order to use DMatrix, the label needs to be numberic, I will convert it first\n",
    "    ## good tutorial: http://ieva.rocks/2016/08/25/iris_dataset_and_xgboost_simple_tutorial/\n",
    "    \n",
    "    t_start = time.time()\n",
    "    d_class_to_int = {}\n",
    "    for i, elem in enumerate(np.unique(y_train)):\n",
    "        d_class_to_int[elem] = i\n",
    "    \n",
    "    ## make the reverse dictionary for later use (if needed)\n",
    "    d_int_to_class = {v: k for k, v in d_class_to_int.items()}\n",
    "    \n",
    "    y_train_xgb = np.vectorize(d_class_to_int.get)(y_train)\n",
    "    y_test_xgb = np.vectorize(d_class_to_int.get)(y_test)\n",
    "    \n",
    "    ## make the train and test set\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train_xgb)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test_xgb)\n",
    "    \n",
    "    ## specify parameters via map\n",
    "    param = {\n",
    "         'max_depth': 50, \n",
    "         'eta': 0.3, # the training step for each iteration \n",
    "         'silent': 0, # logging mode, tell me sth\n",
    "         'objective':'multi:softmax',  # multiclass problem\n",
    "         'num_class': len(d_class_to_int),  # the number of classes that exist in this datset\n",
    "         'nthread': 4, # the number of threads\n",
    "        }\n",
    "    num_round = 1 # the number of training iterations\n",
    "    \n",
    "    ## train!\n",
    "    bst = xgb.train(param, dtrain, num_round)\n",
    "    ## make prediction\n",
    "    y_pred = bst.predict(dtest)\n",
    "    ## to get the accuracy\n",
    "    print('The time of xgboost is: {} minutes'.format((time.time() - t_start)/60))\n",
    "    print('The train accuracy is: ', sum(bst.predict(dtrain) == y_train_xgb) / len(y_train_xgb))\n",
    "    print('The test accuracy is: ', sum(y_pred == y_test_xgb) / len(y_pred))\n",
    "    ## plot the feature importance\n",
    "    #xgb.plot_importance(bst)\n",
    "    #plt.show();\n",
    "\n",
    "    ## to save the model, if needed      \n",
    "    xbg_model_for_output = bst\n",
    "    X_sample = pd.Series(0, index = X_train.columns)\n",
    "    model_name = 'XGB.pkl'\n",
    "    # pickle.dump([model_for_output, scaler, X_sample], open(model_name, 'wb'))\n",
    "    print('Elapsed time is {} min'.format((time.time() - t_start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================== Naive Bayes ==================================\n",
    "if nb_model:\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    nb_clf = GaussianNB()\n",
    "    nb_clf.fit(X_train_scaled, y_train)\n",
    "    print('The test scroe for Naive Bayes is: {}'.format(nb_clf.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
