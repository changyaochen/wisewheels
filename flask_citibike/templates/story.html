
<!DOCTYPE html>
<html lang="en">
<style type="text/css">
    .d3-tip {
    line-height: 1;
    font-weight: bold;
    padding: 8px;
    /*background: rgba(0, 0, 0, 0.8);*/
    background: lightgrey;
    /*color: #fff;*/
    color: black;
    border-radius: 2px;
    }


    /* Navigation highlight for current page */
    body#home a#homenav, body#about a#aboutnav, body#menupage a#menunav, body#contact a#contactnav {
        background-color: #843534;
    }

    .current:after {
        content: ' ';
        position: absolute;
        border: solid 10px transparent;
        border-top: solid 0px transparent;
        border-width: 10px;
        left: 50%;
        margin-left: -10px;
        border-color: #843534 transparent transparent transparent;
    }

    #slides {
    position: absolute;
    z-index: 15;
    top: 50%;
    left: 50%;
    margin: -285px 0 0 -480px;
    background: black;
  }

</style>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
  <meta name="description" content="Given the pickup event of a Citibike, this application will predict what is the most likely neighborhood the bike trip will end in.">
  <meta name="author" content="Changyao Chen">
  <link rel="icon" href="../../favicon.ico">

  <title>WiseWheels</title>

  <!-- Bootstrap core CSS -->
  <link href="../static/css/bootstrap.min.css" rel="stylesheet">

  <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
  <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet">

  <!-- Custom styles for this template -->
  <link href="starter-template.css" rel="stylesheet">

  <!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
  <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->
  <script src="../../assets/js/ie-emulation-modes-warning.js"></script>

  <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
      <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
        <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
        <![endif]-->
      </head>

      <body>

        <nav class="navbar navbar-inverse navbar-fixed-top">
          <div class="container">
            <div class="navbar-header">
              <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
              <a class="navbar-brand" href="{{ url_for('prediction_input') }}">WiseWheels</a>
            </div>
            <div id="navbar" class="collapse navbar-collapse">
              <ul class="nav navbar-nav">
                <li><a href="{{ url_for('prediction_input') }}">Home</a></li>
                <li class="active"><a href="{{ url_for('story') }}">Data Story</a></li>
                <li><a href="{{ url_for('about') }}">About Me</a></li>
                
              </ul>
            </div><!--/.nav-collapse -->
          </div>
        </nav>
        <br><br>
        <div class="container">
        <div class="row">
          <div class="col-md-10">
          <div class="starter-template">
            <p><br>
              <h5><li><a href="#why">Why I am interested.</a><br></h5>
              <h5><li><a href="#data">The 'raw' data.</a><br></h5>
              <h5><li><a href="#goal">The goal, and the benchmarks.</a><br></h5>
              <h5><li><a href="#feature">Feature engineering.</a><br></h5>
              <h5><li><a href="#model">Model selection and performance.</a><br></h5>
              <h5><li><a href="#learn">What I have achieved / learned.</a><br></h5>
              <h5><li><a href="#story1">Back story 1: how I got here.</a><br></h5>
              <h5><li><a href="#story2">Back story 2: the model with 99% accuracy.</a></h5>
              <br>
              In this page, I like to record how this project has evolved. This is for my <a href="http://insightdatascience.com/" target="_blank">Insight Data Science</a> project. The Insight project is designed to be finished in 4 weeks (more like 3 weeks...), from the project idea, to the final deliverable such as this website. I would say the schedule is very tight: to have a good project idea is already not an easy task, not to mention the many technical holes that one often need to fill during the process (such as general machine learning techniques, web-scraping, or front-end development as in my case). There are many late nights, and more importantly, I have the rush in the morning to wake up and get to work, just like in graduate school! I do very much enjoy the process. <br><br>

              <b> <a id="why">Why I am interested.</a> </b>
              <br><br>
              I came to Insight with some vague project ideas in mind, and some exploration with the <a href="https://www.citibikenyc.com/system-data" targe="_blank">Citibike data</a> is one of them, and I have <a href="http://nbviewer.jupyter.org/github/changyaochen/CitiBike/blob/master/Citibike.ipynb" target="_blank">explored</a> the data previously. After the project idea "brain-storm" in the first week, with many ideas came and went (where I learn to embrace the term "fail fast and fail early"), I finally settle down with this Citibike project. I am not a stranger to Citibike: when it was launched in 2013, I was among the first to sign up for the membership, and used it quite often, but it didn't have any station installed close to where I lived (Columbia University), the north-most station only touches the south side of the Central Park. When I return to the city this year, I found that there are stations as far north as 110th street. Considering that a monthly Subway ticket will cost me about the same price as the annual Citibike membership, and Insight's office is right at the center of Flatiron, therefore I immediately signed up and started using Citibike for commuting. <br><br>

              One problem though, is that it is very tricky to find a bike. In the morning, I need to get to the 110th station well before 8 am, before all the bikes run out. There are few times that I need to walk to next bike station (usually ~ 5 blocks away), with the hope to find some bikes are still available. By the time I got to Insight's office, the problem becomes to find an empty dock - the bike station right across Insight is almost always full, luckily there is a large bike station just one block away, hence it is less a concern. The same routine happens for the return trips. Although I don't expect that I can find a Citibike anytime from any station, but I really wish the bike availability can be better addressed, and therefore I started this project. I am not being naive to believe this will bring the magic solution of any kind, there are people devoting a <a href="https://ecommons.cornell.edu/handle/1813/40922" target="_blank">Ph.D. thesis</a> on this problem. Citibike do have put in efforts to better address this re-balancing problem, with various bulk bike transfer protocols. Lately, Citibike just launched <a href="http://bikeangels.citibikenyc.com/" target="_blank">bike angels program</a> to incentivize riders to take bikes from bike-rich stations to bike-poor station. Here, I like to see, with the available data, what can we learn, and hopefully lend some insight to Citibike. <br><br>

              <b> <a id="data">The 'raw' data.</a> </b>
              <br><br>
              The <a href="https://www.citibikenyc.com/system-data", targe="_blank">data</a> available are the trip data, from the beginning (mid-2013) until early this year (2017): there are more than 35 million trips! One (really) nice thing about the Citibike data is that they are very <b>clean</b>: the data is organized monthly, in .csv format, with only few N/A values to deal with. For each trip, the recorded information are: start station (name, id, latitude and longitude), start date / time, end station (name, id, latitude and longitude), trip duration, and bike ID. If the rider is an annual member, then the gender and age are also recorded, otherwise (short-term user) these two fields are not available. Other information about the riders are all erased. Overall, more than 90% of the trips are taken by annual members, and I will only focus my effort data with these annual members. <br><br>

              One nice thing about working with Citibike data is that, there are many demographic data about New York can be easily found, for example, from </a href="https://opendata.cityofnewyork.us/" target="_blank">NYC Open Data</a>, or <a href="https://www.census.gov/" targe="_blank">US Census</a> and <a href="https://www.census.gov/programs-surveys/acs/" target="_blank">American Community Survey</a>. During the course of this project, I've also queried <a href="https://www.yelp.com/developers/documentation/v2/overview" target="_blank">Yelp API</a> for various local information. In the following sections, I will describe how I use (or not use) them.<br><br>

              In this project, I will only use the data from <b>2016</b>, in which there are more than 12 million trips. By doing so, I can leverage the "big data" and a tangible model: it is big enough for me to capture the patterns in the data, but still small enough that my laptop can comfortably handle.<br><br>
              
              <b> <a id="goal">The goal, and the benchmarks.</a> </b>
              <br><br>
              Simply put, the goal of this project is <b>to predict the end neighborhood of a Citibike ride when the bike is picked up by a member</b>. If one can accurately predict the destinations of ongoing trips, then it can better guide the short-term re-balancing plans: the free bikes will be sent to the would-be bike-poor neighborhoods, in order to replenish the bike availability as soon as possible. I am not trying to predict exactly which <b>station</b> the trip will go to: it just sound impossible intuitively: when I am riding a Citibike, I won't be sure which station I will be ended up with (to have available docks). Here I will simply assign the stations to the neighborhoods, whose <a href="https://data.cityofnewyork.us/City-Government/Neighborhood-Names-GIS/99bc-9p23" target="_blank">centroid</a> is closest to that station. Even at neighborhood level, I don't know how well I can do. But let's define a <b>metric</b> to measure the success: the simplest one is just the percentage of right answer (end neighborhood) the model predicts: 0 is the worst and 100% is the best. <br><br> 

              <div class="starter-template">
                    {{ neighborhood_html|safe }}
                    <br>
              </div>

              Before I embark on the modeling process, let me establish some benchmarks for the performance metric. The goal is to correctly predict the end neighborhood, and there are 60 neighborhoods, one benchmark can be the uniformly, randomly guess between all these 60 possible outcomes, which gives me 1.67% accuracy. But this won't make too much sense: why would one guess that different end neighborhoods have the same chance? 

              By looking through all the past trip, one can create a histogram of all the end neighborhoods, therefore, a better guess would be to bet on the most frequent neighborhood. In this case (2016 data), the most frequent end neighborhood is Flatiron, in which 8.9% of the trips end. Here I established the first benchmark: <b>8.9%</b>.<br><br>

              Let me improve my guess a bit more: the median travel time for Citibike riders is about <a href="http://nbviewer.jupyter.org/github/changyaochen/CitiBike/blob/master/Citibike.ipynb" target="_blank">10 minutes</a>, so it is reasonable to guess the trip will end up in the <b>same</b> neighborhood as the start station, or end up in the <b>closest</b> neighborhood from the start station. The former guess gives me an accuracy of <b>13.2%</b>, and the latter gives me <b>11.8%</b>.<br><br>

              The last benchmark I come up with is to make a more educated guess, by looking at the trip history of individual stations. Then for a new trip that starts from this station, I will always guess it will go to the most frequent end neighborhood for this station. This benchmark turns out to be surprisingly good, at <b>19.2%</b>, and at the end I only manage to barely beat this benchmark.<br><br>

              <b> <a id="feature">Feature engineering.</a> </b><br><br>
              Given the fact I want to 'predict' a rider's behavior (where is he or she going), I need to know more about this the rider. From the raw trip data, the information I have about the rider are: gender, age, and the fact he or she picks up a bike from one specific station at a specific date / time. How can I differentiate riders having the exact same information as above? One possible 'differentiator' can be weather: those 'identical' riders might have different weather preference. I requested the <a href="https://www.ncdc.noaa.gov/cdo-web/" target="_blank">historical daily weather data</a> for New York City from National Oceanic and Atmospheric Administration, and attached the daily weather conditions to the existing trips. 

              Although we, as human, know very clearly the difference between different stations - one from Grand Central seems quite different from the one from 110th Street: the former is much busier. But how would the model know? All it sees now is just a station id. Therefore, I attempt to add some station-specific features, to 'guide' the model to find the right answers. In essence, I want to build a 'fingerprint' for each station. The ideas that I've come up with include: the distance to other stations, and other neighborhood centroids; the histogram of the past end neighborhoods for each station; the <a href="https://data.cityofnewyork.us/City-Government/New-York-City-Population-By-Census-Tracts/37cg-gxjd" target="_blank">population</a> and <a href="https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?src=bkmk" target="_blank">median house income</a> of the closest census block to each station; and numbers of various kinds of business within 250-meter radius (average distance between stations), queried from Yelp API. However, it turns out only the distance between stations, and the end neighborhood history made significant differences.<br><br>

              What I really wish, is the historical trips of individual rider, similar to the browser cookie for a web-surfer. Citibike must have it, but I think I've done what could be done from my end...<br><br>

              <b> <a id="model">Model selection and performance.</a> </b><br><br>

              Now I am geared with all the features, old and new, useful and useless (which I don't know yet), it comes to the time to find and tune the best model. This problem is a typical multi-class classification problem, with more than 10 million sample, with hundreds of features (after one-hot encoding), and 60 classes. The package I used is sklearn (also known as <a href="http://scikit-learn.org/stable/" target="_blank">scikit-learn</a>), and <a href="https://xgboost.readthedocs.io/en/latest/" traget="_blank">xgboost</a>. I first tried the usual candidates, namely, logistic regression, support vector machine, random forest, and gradient boosted trees (with xgboost).<br><br>

              But the first problem I am facing is how to handle the 10 million samples. My laptop can load all of them in, but running some regression models on all of the 10 million samples almost always killed the jupyter notebook. Therefore, for the majority of the model selection, I only randomly sample <b>1%</b> of the total data (about 100,000 samples), and then I split this data 80/20 for training and test purposes. Whenever the option of stochastic gradient descent (SGD) is available (for logistic regression and support vector machine), I will adopt SGD for the efficiency. <br><br>

              To combine both the feature and model selection, I am doing a manual gridserch: starting from the bare features (directly obtained from the trip data), all the four models are trained and validated (against test data), and then add a new set of features in. Occasionally, I will tune the hyperparameters, but mostly with gut intuitions. I've also found that it is quite hard to track the progress of xgboost, I have to run couple fittings with small sample sizes to extrapolate the expected time for the 'real' problem. But this is my first time using xgboost for a real problem (aside from tutorials), it might just be my inexperience. 

              <div class="starter-template">
                    <br>
                    {{ score_html|safe }}
              </div>

              From the results, it seems that random forest (from sklearn) and gradient boosted trees (from xgboost) give the similar results, that are around 20%. In the end I choose random forest as the backend model since it seems to run a bit faster, and can be parallelized. I am also told that xgboost is quite hard to get the right parameters, therefore, with more time, it might give me slightly better performance. Below is the confusion matrix for the deployed model (random forest).<br><br>

              <div class="starter-template">
                    {{ cm_html|safe }}
                    <br>
              </div>

              Now I am handed with this model with 20% accuracy, and looking back at the 19% benchmark, an inevitable question comes to me: why am I doing this? Why don't one just simply go back and rely on the historical patterns to make future prediction? Of course that is a very valid proposition, I did try the Gaussian Naive Bayes classifier provided by sklearn, but the accuracy is just few percent. What is more, here we achieved the flexibility and the extensibility: if a new trip starts with totally new profile so that discrete look-up method fails, any of the model shown above will be able to handle that. If more data become available, be it the detailed rider profile, the existing data pipeline can be directly applied.<br><br>

              What is more, if I can exploit the total 10 million samples, instead of just the <b>1%</b> that I am currently using now, there might be a boost to the accuracy. There might be a point, beyond which more samples won't bring proportionally more return, however, that is not something I can easily to access at this point. It is definitely worth looking. 

              <b> <a id="learn">What I have achieved / learned.</a> </b>
              <br><br> 

              Looking back for the last 4 weeks, two things struck me. First, how fast the 4 weeks have come by: the weekly roadmap outlined by Emily (one of our program directors) is still very much vivid in my head, and the pointer is still at week 1, day 1. Since then, I, like many other fellow Insight Fellows are failing at various project ideas, hammering out minimum viable projects (MVPs), practicing demos, working together late to midnights, just to get the websites live, and we have done that! Secondly, I am struck by how much I've learned within such short period of time. The Insight staffs are immensely helpful, and the schedules are carefully crafted (and crazy of course). Getting through the whole process, from data collection to the frontend deployment really helped me to better understand what lies ahead, waiting for me. <br><br>

              <b> <a id="story1">Back story 1: how I got here.</a> </b>
              <br><br> 
              I want to record all my failed project ideas here. But this is less fun, let me work on the next section first.<br><br>

              <b><a id="story2">Back story 2: the model with 99% accuracy.</a></b>
              <br><br>

              For the fellows who have sit in my previous demos, they might be surprised by my claim of 99% accuracy of the model. That is right, <b>99%</b>, not 20%. I didn't pull this number out from my hat, I genuinely believe in it! Both the random forest and boosted trees give me the same answer. I tried different portions of the data, still holds! What makes it more believable is that, I can see the accuracies of the models gradually improve as I add more features in, that is like the ultimate feature engineering (what was the Wednesday of week 3, I then allow myself an 8-hour sleep that night, problem solved!). Look how smart I am! Despite warnings and doubts from Insight alums and staffs, I even came up with the explanation that, there are only 200,000 unique members riding those 10 million trips, and my model is able to identify their behavior very well. Almost convinced everyone, myself included. Not matter how extraordinary the claim is, I am in heaven. <br><br>

              Then on the flight back to Chicago, on the Friday of week 4, I found something doesn't add up: while putting drastically different input to the website, I always get similar results. I went into fully debugging mode, tracing one step back at a time, and finally I found the culprit: I made a mistake at the very beginning, when I am making the data set. Instead of merging two tables on the 'start station id' key, I merged them on the 'end station id' key. By doing so, some information about the answer is leaked to the features. Given the power of the appropriate models (I guess I can be proud of this part...), they will of course get the 99% accuracy - it should get 100% accuracy!<br><br>

              By the time the plane landed, my heart sinks to the bottom of Michigan lake, it is the scenario of the recent play (<a href="http://www.chicagotribune.com/entertainment/theater/reviews/ct-queen-victory-gardens-review-ott-0428-20170425-column.html" target="_blank">Queen</a>) we saw in real life! Over the weekend, I was working with my wife (who can be a much better Data Scientist than I will ever be, if she chose to do so) to find a way to salvage the project. Gladly I've all the data pipeline established, we quickly tried few more ideas. Sadly, I came to the conclusion that, with limited information about the rider, there doesn't seem to be other ways to differentiate them. Intuitively, if a human can not make a sensible prediction with such information, how would a model can?<br><br>

              Well lesson learned: models with 99% accuracy is possible (think MNIST), but without clear verification that a human can achieve the same level of performance with the same information, we need to be very skeptical. <br><br>

              So long, the model with 99% accuracy.<br><br></p>
          </div>
        </div>
        </div>
        </div>

                    <script src="static/js/bootstrap.min.js"></script>
                    <script src="https://code.jquery.com/jquery-1.10.2.min.js"></script>


      <!-- Bootstrap core JavaScript
      ================================================== -->
      <!-- Placed at the end of the document so the pages load faster -->
      <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
      <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
      <script src="../../dist/js/bootstrap.min.js"></script>
      <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
      <script src="../../assets/js/ie10-viewport-bug-workaround.js"></script>
      <!--<script src="static/js/bootstrap.min.js"></script>-->
      <script src="static/js/bootstrap.js"></script>  
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap-datepicker/1.4.1/js/bootstrap-datepicker.min.js"></script>
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap-datepicker/1.4.1/css/bootstrap-datepicker3.css"/>
      
      <script>
      $(document).ready(function(){
      var date_input=$('input[name="date"]'); //our date input has the name "date"
      var container=$('.bootstrap-iso form').length>0 ? $('.bootstrap-iso form').parent() : "body";
      var options={
        format: 'mm/dd/yyyy',
        container: container,
        todayHighlight: true,
        autoclose: true,
      };
      date_input.datepicker(options);
    })
      </script>
    </body>
    </html>
